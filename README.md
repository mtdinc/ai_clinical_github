# Project: AI Clinical Cases Result Addition

This project focuses on the systematic analysis and evaluation of Large Language Model (LLM) responses to clinical case vignettes. The core methodology involves presenting LLMs with clinical information in a staged manner, simulating the sequential disclosure of patient data encountered in real-world medical diagnostics. The primary goal is to assess how different LLMs process evolving case details, formulate differential diagnoses, and arrive at final diagnoses, and to quantify their performance against established true diagnoses.

## Directory Structure

The project is organized into the following directories:

*   **`data/`**: Contains the clinical case vignettes (`.txt` files) organized into `cases_CPS/` and `cases_typical/` subdirectories. Within these, cases are further organized into `stage_1/`, `stage_2/`, and `stage_3/` subdirectories, representing increasing levels of information. Each case type also includes a corresponding `cases_<type>_case_list.csv` with true diagnoses.
*   **`src/`**: Contains the Python and R scripts used for the project:
    *   **`src/python/`**: Python scripts for processing cases, generating LLM responses, merging data, and evaluating scores.
    *   **`src/R/`**: R scripts for visualizing the results.
*   **`results/`**: Contains the final output files, including performance metrics and plots generated by the analysis and visualization scripts.
*   **`documentation.md`**: Provides additional documentation about the project.

## Purpose

The core purpose of this project is to:
1.  Evaluate the diagnostic capabilities of various LLMs (including models from Anthropic, OpenAI, and Google) when presented with clinical vignettes.
2.  Analyze how LLM performance changes with sequential disclosure of information (staged approach).
3.  Programmatically score LLM-generated differential diagnoses against true medical diagnoses using top-k accuracy metrics.
4.  Provide a structured dataset of LLM responses and their evaluations for further research.

## Workflow Overview

1.  **Data Input**: Clinical cases (`.txt` files) and true diagnoses (`*_case_list.csv`) are located in the `data/` directory.
2.  **LLM Response Generation**: Python scripts in `src/python/` process the `.txt` cases through multiple LLMs and save their responses.
3.  **Data Aggregation & Transformation**: Python scripts in `src/python/` merge the LLM responses, add true diagnoses, and transform the data into formats suitable for analysis.
4.  **Automated Scoring**: Python scripts in `src/python/` use an LLM to evaluate the responses, adding top-k accuracy scores (k=1, 5, 10) for differential diagnoses.
5.  **Visualization**: R scripts in `src/R/` generate plots and tables from the results.

## Automated Pipeline

The project includes an orchestrator script (`src/python/run_pipeline.py`) that automates the entire workflow for a given dataset.

**Before running the pipeline, ensure you have set the necessary environment variables for the LLM API keys:**

```bash
export ANTHROPIC_API_KEY='your_anthropic_api_key'
export OPENAI_API_KEY='your_openai_api_key'
export GOOGLE_API_KEY='your_google_api_key'
```

Replace `'your_anthropic_api_key'`, `'your_openai_api_key'`, and `'your_google_api_key'` with your actual API keys.

To run the pipeline:

```bash
# Process all stages for a dataset (e.g., cases_typical)
python src/python/run_pipeline.py cases_typical

# Process specific stages only
python src/python/run_pipeline.py cases_typical --stages stage_1 stage_3

# Skip specific steps in the pipeline
python src/python/run_pipeline.py cases_typical --skip-unified
```

This script:
1. Accepts a dataset folder name as input (e.g., `cases_typical`).
2. Iterates through all stages (`stage_1`, `stage_2`, `stage_3`) or specified stages within the `data/` directory.
3. For each stage, sequentially runs the necessary Python scripts in `src/python/`.
4. Provides options to skip specific steps if needed (useful for resuming interrupted runs).

## LLMs and Evaluation

The project utilizes models such as various versions of Anthropic's Claude, OpenAI's GPT series (including O1 models), and Google's Gemini. Evaluation is performed by comparing LLM outputs to known true diagnoses, with scoring facilitated by an independent LLM (e.g., Claude 3.7 Sonnet) to assess the correctness of differential diagnoses using top-k accuracy metrics (k=1, 5, 10).

## Visualization

The project includes R scripts in `src/R/` for visualizing the results of the LLM evaluation pipeline.

To run the visualization scripts:

```bash
# Make sure you have R installed with required packages
# (dplyr, ggplot2, tidyr, forcats)

# Example: Run a specific visualization script
Rscript src/R/ggplot_stage3_top_k.R
```

The visualization scripts will save the plots as PDF and PNG files in the `results/` directory, along with CSV files containing the performance metrics.

See `src/R/README.md` for more details on the visualization scripts.

(To be expanded with more specific information about the methodologies, LLMs used, analysis techniques, and key findings.)
