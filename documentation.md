# Project Documentation: AI Clinical Cases Result Addition

## 1. Introduction/Overview

This project is designed to analyze and evaluate the performance of various Large Language Models (LLMs) in the context of clinical case vignettes. The core methodology involves presenting LLMs with clinical information in a staged manner, simulating the sequential disclosure of patient data in a real-world medical scenario. The project aims to understand how LLMs process evolving case details, formulate differential diagnoses, and arrive at final diagnoses. The performance is then programmatically scored against true diagnoses.

## 2. File Structure

The project's primary files and data are located within the `/` (root) and `src/` directories.

*   **`/README.md`**: Provides a general overview of the project for users.
*   **`/documentation.md`**: This file, providing a detailed technical overview of the project, its components, and workflow.
*   **`/journal_reviewer_requests.md`**: (Purpose to be clarified, likely related to publication or peer review notes).

*   **`src/`**: Contains all data, scripts, and results.
    *   **`cases_test/`**: Contains test case data.
        *   `stage_1/`, `stage_2/`, `stage_3/`: Subdirectories holding `.txt` files for different stages of information disclosure for test cases. Each `.txt` file represents a clinical vignette at a specific stage.
        *   `cases_test_case_list.csv`: CSV file containing the list of test cases and their corresponding true diagnoses (`true_dx`).
    *   **`cases_typical/`**: Contains "typical" or standard clinical case data.
        *   `stage_1/`, `stage_2/`, `stage_3/`: Similar staged `.txt` files for typical cases.
        *   `cases_typical_case_list.csv`: CSV file with the list and true diagnoses for typical cases.
    *   **`cases_CPS/`**: Contains clinical problem-solving (CPS) case data.
        *   `Stage3/`: Subdirectory holding `.txt` files for CPS cases, seemingly focused on a single stage (Stage 3). (Further details on how `cases_CPS_case_list.csv` might be named or structured would be beneficial).
    *   **Python Scripts (`.py` files):**
        *   `unified_models_v2.py`: Script to process `.txt` case files, query various LLMs, and save their responses.
        *   `column_merger_nejm_ordered.py`: Script to merge responses from different LLMs, add true diagnoses, and transform data into wide and long formats.
        *   `score_evaluater_inbatches.py`: Script to evaluate LLM responses against true diagnoses using another LLM for scoring.
    *   **CSV Files (`.csv` files in `src/`):**
        *   `llm_<provider>_typical_responses_<dataset>_<stage>.csv` (e.g., `llm_openai_typical_responses_cases_test_stage_3.csv`): Raw responses from a specific LLM provider for a given dataset and stage. Generated by `unified_models_v2.py`.
        *   `merged_llm_responses_<dataset>_<stage>_ordered.csv` (e.g., `merged_llm_responses_cases_test_stage_3_ordered.csv`): Merged responses from all LLMs in a wide format. Generated by `column_merger_nejm_ordered.py`.
        *   `Results_typical_<dataset>_<stage>_long_format_ordered.csv` (e.g., `Results_typical_cases_test_stage_3_long_format_ordered.csv`): Merged responses transformed into a long format. Generated by `column_merger_nejm_ordered.py`. This is the input for the scoring script.
        *   `Results_typical_<dataset>_<stage>_long_format_ordered_top_k_rated.csv`: The long format data with added columns for top-k accuracy scores. Generated by `score_evaluater_inbatches.py`.

## 3. Data Flow / Workflow

The project follows a sequential data processing pipeline:

1.  **Case Preparation**: Clinical vignettes are prepared as `.txt` files and organized into stages within `src/cases_test/`, `src/cases_typical/`, and `src/cases_CPS/`. Corresponding `*_case_list.csv` files store the true diagnosis for each case.

2.  **LLM Response Generation (`unified_models_v2.py`)**:
    *   The script reads `.txt` case files from a specified directory (e.g., `src/cases_test/stage_3`).
    *   It iterates through configured LLM providers (Anthropic, OpenAI, Google) and their specified models.
    *   For each case and model, it sends the case text as a prompt to the LLM.
    *   The LLM responses are collected and saved into provider-specific CSV files (e.g., `llm_anthropic_typical_responses_cases_test_stage_3.csv`).

3.  **Data Merging and Transformation (`column_merger_nejm_ordered.py`)**:
    *   This script reads the individual provider CSVs generated in the previous step.
    *   It merges these files based on `case_number`.
    *   It reads the relevant `*_case_list.csv` to append the `true_dx` (true diagnosis) for each case.
    *   A `stage` column is added.
    *   The combined data is first saved in a wide format (`merged_llm_responses_..._ordered.csv`).
    *   The wide-format data is then transformed into a long format (`Results_typical_..._long_format_ordered.csv`), where each row represents a single model's response to a single case. This format is suitable for analysis and scoring.

4.  **Response Evaluation (`score_evaluater_inbatches.py`)**:
    *   This script takes the long-format CSV (e.g., `Results_typical_cases_test_stage_3_long_format_ordered.csv`) as input.
    *   For each LLM response in the file, it uses another LLM (specifically Anthropic's Claude 3.7 Sonnet) to score the response.
    *   Three top-k accuracy scores are generated:
        *   `differential_top_1_accuracy`: Score (0 or 1) indicating if the `true_dx` (or a clinically related diagnosis) is found as the first diagnosis in the LLM's differential list.
        *   `differential_top_5_accuracy`: Score (0 or 1) indicating if the `true_dx` (or a clinically related diagnosis) is found within the first five diagnoses in the LLM's differential list.
        *   `differential_top_10_accuracy`: Score (0 or 1) indicating if the `true_dx` (or a clinically related diagnosis) is found within the first ten diagnoses in the LLM's differential list.
    *   The script processes data in batches and appends these scores to the input CSV, saving the output as a `..._top_k_rated.csv` file (e.g., `Results_typical_cases_test_stage_3_long_format_ordered_top_k_rated.csv`).

## 4. Scripts Overview

### 4.1. `src/unified_models_v2.py`

*   **Purpose**: Automates the process of sending clinical case texts to various LLMs and collecting their responses.
*   **Key Inputs**:
    *   API keys for Anthropic, OpenAI, and Google (hardcoded or environment variables).
    *   `--medical_cases_dir`: Command-line argument defining the input directory for `.txt` case files (e.g., `cases_typical_concise_selected/stage_1`).
    *   Model names and configurations for each provider (defined in `providers_config` dictionary).
*   **Usage**:
    ```bash
    python src/unified_models_v2.py --medical_cases_dir cases_typical_concise_selected/stage_1
    ```
*   **Process**:
    *   Reads all `.txt` files from `MEDICAL_CASES_DIR`.
    *   Uses `ThreadPoolExecutor` for parallel processing of providers and models.
    *   Constructs a standardized prompt for the LLMs.
    *   Handles API calls to Anthropic, OpenAI (including O1 models), and Google AI.
    *   Includes specific handling for Anthropic's "thinking" feature and OpenAI O1's "reasoning_effort".
*   **Key Outputs**: Separate CSV files for each LLM provider, containing case ID, case text, and model responses (e.g., `llm_openai_typical_responses_cases_test_stage_3.csv`).

### 4.2. `src/column_merger_nejm_ordered.py`

*   **Purpose**: Consolidates individual LLM response CSVs into a single dataset, adds true diagnoses, and restructures the data into wide and long formats.
*   **Key Inputs**:
    *   Provider-specific CSV files generated by `unified_models_v2.py`.
    *   `*_case_list.csv` (e.g., `cases_typical_concise_selected_case_list.csv`) containing true diagnoses. Path and encoding detection logic is included.
    *   Command-line arguments: `--dataset` (e.g., "cases_typical_concise_selected") and `--stage` (e.g., "stage_1").
*   **Usage**:
    ```bash
    python src/column_merger_nejm_ordered.py --dataset cases_typical_concise_selected --stage stage_1
    ```
*   **Process**:
    *   Reads and sorts each provider's CSV by `case_number`.
    *   Merges these DataFrames.
    *   Adds a `stage` column.
    *   Merges `true_dx` from the case list file.
    *   Saves the merged data in wide format.
    *   Pivots the wide data to a long format using `pd.melt`.
*   **Key Outputs**:
    *   `merged_llm_responses_<DATASET>_<STAGE>_ordered.csv` (wide format).
    *   `Results_typical_<DATASET>_<STAGE>_long_format_ordered.csv` (long format).

### 4.3. `src/score_evaluater_inbatches.py`

*   **Purpose**: Programmatically scores LLM responses for correctness of differential diagnoses using top-k accuracy metrics (k=1, 5, 10).
*   **Key Inputs**:
    *   Long-format CSV file (e.g., `Results_typical_cases_typical_concise_selected_stage_1_long_format_ordered.csv`) generated by `column_merger_nejm_ordered.py`.
    *   Anthropic API key (hardcoded).
    *   Command-line arguments: `--dataset` (e.g., "cases_typical_concise_selected") and `--stage` (e.g., "stage_1").
*   **Usage**:
    ```bash
    python src/score_evaluater_inbatches.py --dataset cases_typical_concise_selected --stage stage_1
    ```

### 4.4. `src/run_pipeline.py`

*   **Purpose**: Orchestrates the execution of the three main scripts in sequence for a given dataset and its stages.
*   **Key Inputs**:
    *   Dataset folder name (e.g., `cases_typical_concise_selected`).
    *   Optional arguments:
        *   `--stages`: List of stages to process (default: `stage_1 stage_2 stage_3`).
        *   `--skip-unified`: Skip running `unified_models_v2.py`.
        *   `--skip-merger`: Skip running `column_merger_nejm_ordered.py`.
        *   `--skip-scorer`: Skip running `score_evaluater_inbatches.py`.
*   **Process**:
    *   For each stage in the dataset, runs the three scripts in sequence.
    *   Checks if each stage exists in the dataset before processing.
    *   Provides detailed logging of each step.
*   **Usage**:
    ```bash
    # Process all stages for a dataset
    python src/run_pipeline.py cases_typical_concise_selected

    # Process specific stages only
    python src/run_pipeline.py cases_typical_concise_selected --stages stage_1 stage_3

    # Skip specific steps in the pipeline
    python src/run_pipeline.py cases_typical_concise_selected --skip-unified
    ```
*   **Process**:
    *   Reads the input CSV using pandas.
    *   Uses `asyncio` and `aiohttp` for asynchronous API calls to Anthropic's Claude 3.7 Sonnet model.
    *   For each row (LLM response), it constructs three prompts for the scoring LLM to evaluate top-k accuracy:
        *   One for top-1 accuracy (k=1)
        *   One for top-5 accuracy (k=5)
        *   One for top-10 accuracy (k=10)
    *   For each prompt, the scoring LLM evaluates if the true diagnosis (or a clinically related diagnosis) appears within the top-k diagnoses in the differential list.
    *   The scoring LLM returns a 0 or 1 for each k value.
    *   Processes rows in batches to manage API rate limits and save progress.
*   **Key Outputs**: A CSV file with `_top_k_rated` appended to the input filename, containing the original data plus three new score columns: `differential_top_1_accuracy`, `differential_top_5_accuracy`, and `differential_top_10_accuracy` (e.g., `Results_typical_cases_CPS_Stage3_long_format_ordered_top_k_rated.csv`).

## 5. Key Libraries Used

### 5.1. Python Libraries

*   **`pandas`**: For data manipulation and CSV file operations across all scripts.
*   **`anthropic`**: Python SDK for interacting with Anthropic's LLMs (Claude models). Used in `unified_models_v2.py` and `score_evaluater_inbatches.py`.
*   **`openai`**: Python SDK for interacting with OpenAI's LLMs (GPT models, O1 models). Used in `unified_models_v2.py`.
*   **`google.generativeai`**: Python SDK for interacting with Google's Generative AI models (Gemini models). Used in `unified_models_v2.py`.
*   **`asyncio`**: For asynchronous programming, primarily in `score_evaluater_inbatches.py` to handle API calls efficiently.
*   **`aiohttp`**: Asynchronous HTTP client/server framework (likely used by `anthropic` async client).
*   **`os`**: For interacting with the file system (e.g., listing files, joining paths). Used in `unified_models_v2.py`.
*   **`csv`**: For CSV file writing operations. Used in `unified_models_v2.py`.
*   **`re`**: For regular expression operations, used in `column_merger_nejm_ordered.py` for extracting case numbers.
*   **`concurrent.futures.ThreadPoolExecutor`**: For parallel execution of tasks, used in `unified_models_v2.py`.
*   **`typing`**: For type hinting.
*   **`functools.partial`**: (Imported in `score_evaluater_inbatches.py`, usage not immediately visible in the provided snippet but might be used in a larger context).
*   **`argparse`**: For parsing command-line arguments in all scripts.

### 5.2. R Libraries

*   **`dplyr`**: For data manipulation and transformation in R visualization scripts.
*   **`ggplot2`**: For creating visualizations in R.
*   **`tidyr`**: For data reshaping and cleaning in R.
*   **`forcats`**: For working with categorical variables (factors) in R.
*   **`scales`**: For formatting axis labels and legends in ggplot2.
*   **`patchwork`**: For combining multiple ggplot2 plots into a single figure with shared legends and consistent formatting.

## 6. Visualization Scripts

### 6.1. `src/R/ggplot_stage2_typical.R`

*   **Purpose**: Creates a point-range plot comparing model performance across Stage 1 and Stage 2, with separate facets for Primary Diagnosis and Differential Diagnosis metrics.
*   **Key Inputs**:
    *   `data2_typical/Results_typical_cases_typical_stage_1_long_format_ordered_rated.csv`
    *   `data2_typical/Results_typical_cases_typical_stage_2_long_format_ordered_rated.csv`
*   **Process**:
    *   Reads and combines data from both stages.
    *   Maps technical model names to simplified display names.
    *   Calculates means and standard errors for each model and stage.
    *   Ranks models based on a weighted score of final diagnosis correctness and differential diagnosis inclusion.
    *   Creates a point-range plot with models on the x-axis and accuracy scores on the y-axis.
*   **Key Outputs**:
    *   `results/model_performance_comparison_vertical.pdf`
    *   `results/model_performance_comparison_vertical.png`
    *   `results/model_performance_table.csv`

### 6.2. `src/R/ggplot_stage3_top_k.R`

*   **Purpose**: Creates a comprehensive bar plot comparing model performance across all three stages (Stage 1, Stage 2, and Stage 3) for the Top-K accuracy metrics (K=1, K=5, K=10) side-by-side.
*   **Key Inputs**:
    *   `Results_typical_cases_test_stage_1_long_format_ordered_top_k_rated.csv`
    *   `Results_typical_cases_test_stage_2_long_format_ordered_top_k_rated.csv`
    *   `Results_typical_cases_test_stage_3_long_format_ordered_top_k_rated.csv`
*   **Process**:
    *   Reads data from all three stages.
    *   Maps technical model names to simplified display names.
    *   Calculates means and standard errors for each model, stage, and K-metric.
    *   Ranks models based on their Top-1 accuracy at Stage 3.
    *   Creates a single plot with models on the x-axis.
    *   For each model, displays three side-by-side bars representing K1, K5, and K10 accuracy.
    *   Each bar shows layered segments for Stage 1, Stage 2, and Stage 3 contributions.
    *   The layering visually demonstrates the incremental improvement in accuracy as more clinical information becomes available.
    *   Adds error bars for Stage 3 (final) scores.
    *   Includes percentage labels above each bar for Stage 3 scores.
*   **Key Outputs**:
    *   `results/model_performance_top_k_comparison.pdf`
    *   `results/model_performance_top_k_comparison.png`
    *   `results/model_performance_top_k_table.csv`

### 6.3. `src/R/test_ggplot_stage3_top_k.R` and `src/R/ggplot_stage3_top_k_test.R`

*   **Purpose**: Generates sample data and creates a test visualization to verify the functionality of the `ggplot_stage3_top_k.R` script.
*   **Process**:
    *   `test_ggplot_stage3_top_k.R`:
        *   Creates a `test_data` directory.
        *   Generates sample data for each stage with simulated accuracy scores.
        *   Simulates improvement across stages and model-specific performance patterns.
    *   `ggplot_stage3_top_k_test.R`:
        *   Reads the sample data.
        *   Creates the visualization using the same approach as the main script.
        *   Outputs to a `test_results` directory instead of `results`.
*   **Key Outputs**:
    *   `test_data/Results_typical_cases_test_stage_1_long_format_ordered_top_k_rated.csv`
    *   `test_data/Results_typical_cases_test_stage_2_long_format_ordered_top_k_rated.csv`
    *   `test_data/Results_typical_cases_test_stage_3_long_format_ordered_top_k_rated.csv`
    *   `test_results/model_performance_top_k_comparison.pdf`
    *   `test_results/model_performance_top_k_comparison.png`
    *   `test_results/model_performance_top_k_table.csv`

### 6.4. `src/R/run_test_visualization.sh`

*   **Purpose**: Automates the testing process for the visualization scripts.
*   **Process**:
    *   Checks if R is installed with the required packages.
    *   Runs `test_ggplot_stage3_top_k.R` to generate sample data.
    *   Runs `ggplot_stage3_top_k_test.R` to create the test visualization.
    *   Verifies that the output files were created successfully.
    *   Opens the PDF visualization for immediate viewing.
*   **Key Features**:
    *   Platform-independent (works on macOS, Linux, and Windows).
    *   Provides clear error messages if any step fails.
    *   Offers a quick way to verify visualization changes without using real data.

### 6.5. `src/R/ggplot_combined_cases.R`

*   **Purpose**: Creates a combined visualization that vertically stacks two plots: one for Complex Cases (Stages 1, 2, 3) and one for Common Cases (Stages 1, 2), with a shared legend and consistent formatting for publication.
*   **Key Inputs**:
    *   `Results_typical_cases_CPS_concise_selected_stage_1_long_format_ordered_top_k_rated.csv`
    *   `Results_typical_cases_CPS_concise_selected_stage_2_long_format_ordered_top_k_rated.csv`
    *   `Results_typical_cases_CPS_concise_selected_stage_3_long_format_ordered_top_k_rated.csv`
    *   `Results_typical_cases_typical_concise_selected_stage_1_long_format_ordered_top_k_rated.csv`
    *   `Results_typical_cases_typical_concise_selected_stage_2_long_format_ordered_top_k_rated.csv`
*   **Process**:
    *   Defines a reusable function `generate_model_plot()` that creates a standardized plot for a given dataset and set of stages.
    *   For each dataset (Complex Cases and Common Cases):
        *   Reads data from all relevant stages.
        *   Maps technical model names to simplified display names.
        *   Calculates means and standard errors for each model, stage, and K-metric.
        *   Ranks models based on a weighted point system that considers performance across all stages and K-metrics.
        *   Creates a plot with models on the x-axis, ordered by their weighted ranking.
        *   For each model, displays three side-by-side bars representing K1, K5, and K10 accuracy.
        *   Each bar shows layered segments for the stages, visually demonstrating incremental improvement.
        *   Adds error bars and percentage labels for the final stage scores.
    *   Uses the `patchwork` package to:
        *   Combine both plots vertically.
        *   Add appropriate titles for each section ("Complex Cases" and "Common Cases").
        *   Create a single, shared legend for the Information Stages.
        *   Apply consistent formatting and styling across both plots.
*   **Key Outputs**:
    *   `results_simplified_weighted_rank/combined_model_performance_comparison.pdf`
    *   `results_simplified_weighted_rank/combined_model_performance_comparison.png`
    *   `results_simplified_weighted_rank/combined_model_performance_table.csv` (Combined summary statistics)
*   **Key Features**:
    *   Creates a publication-ready figure that compares model performance across different case types.
    *   Maintains consistent color schemes, scales, and formatting between the two plots.
    *   Uses a weighted ranking system to order models consistently within each plot.
    *   Provides a compact visualization with a single, shared legend to maximize data-to-ink ratio.

## 7. Data Flow Diagram (Mermaid)

```mermaid
graph TD
    subgraph InputDataPreparation
        direction LR
        TXT_Cases["Case Vignettes (.txt files in stages e.g., cases_typical_concise_selected/stage_1/)"]
        CaseListCSV["True Diagnoses (e.g., cases_typical_concise_selected_case_list.csv)"]
    end

    subgraph Orchestration
        direction LR
        RP["src/run_pipeline.py"]
        Dataset["Dataset Name (e.g., cases_typical_concise_selected)"]
        Dataset --> RP
        RP --> UM
        RP --> CM
        RP --> SE
    end

    subgraph LLMResponseGeneration
        direction LR
        UM["src/unified_models_v2.py"]
        TXT_Cases --> UM
        UM --> AnthropicCSV["llm_anthropic_...csv"]
        UM --> OpenAICSV["llm_openai_...csv"]
        UM --> OpenAI_O1_CSV["llm_openai_o1_...csv"]
        UM --> GoogleCSV["llm_google_...csv"]
    end

    subgraph MergingAndTransformation
        direction LR
        CM["src/column_merger_nejm_ordered.py"]
        AnthropicCSV --> CM
        OpenAICSV --> CM
        OpenAI_O1_CSV --> CM
        GoogleCSV --> CM
        CaseListCSV --> CM
        CM --> MergedWideCSV["merged_llm_responses_..._ordered.csv (Wide Format)"]
        CM --> MergedLongCSV["Results_typical_..._long_format_ordered.csv (Long Format)"]
    end

    subgraph Evaluation
        direction LR
        SE["src/score_evaluater_inbatches.py"]
        MergedLongCSV --> SE
        SE --> RatedCSV["Results_typical_..._long_format_ordered_top_k_rated.csv (Top-K Rated Long Format)"]
    end

    style InputDataPreparation fill:#f9f,stroke:#333,stroke-width:2px
    style Orchestration fill:#ff9,stroke:#333,stroke-width:2px
    style LLMResponseGeneration fill:#ccf,stroke:#333,stroke-width:2px
    style MergingAndTransformation fill:#cfc,stroke:#333,stroke-width:2px
    style Evaluation fill:#ffc,stroke:#333,stroke-width:2px
